# Custom OpenAI Compatible Model Configuration
# Copy this file to .env and modify the values as needed

# API Keys
TAVILY_API_KEY=
RESEARCH_MODEL_CONFIG=
SUMMARIZATION_MODEL_CONFIG=
COMPRESSION_MODEL_CONFIG=
FINAL_REPORT_MODEL_CONFIG=
# Custom OpenAI Compatible Endpoints
# These environment variables allow you to use any OpenAI-compatible API
# You can use either JSON format for full ModelConfig or simple provider:model format

# Example 1: Using OpenRouter with custom models
# RESEARCH_MODEL_CONFIG='{"provider": "openai", "model": "anthropic/claude-3.5-sonnet", "base_url": "https://openrouter.ai/api/v1", "api_key": "your-openrouter-key"}'
# SUMMARIZATION_MODEL_CONFIG='{"provider": "openai", "model": "google/gemini-1.5-flash", "base_url": "https://openrouter.ai/api/v1", "api_key": "your-openrouter-key"}'

# Example 2: Using local Ollama
# RESEARCH_MODEL_CONFIG='{"provider": "openai", "model": "llama3.1:70b", "base_url": "http://localhost:11434/v1", "api_key": "ollama"}'
# SUMMARIZATION_MODEL_CONFIG='{"provider": "openai", "model": "mistral:latest", "base_url": "http://localhost:11434/v1", "api_key": "ollama"}'

# Example 3: Using Together.ai
# RESEARCH_MODEL_CONFIG='{"provider": "openai", "model": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo", "base_url": "https://api.together.xyz/v1", "api_key": "your-together-key"}'
# SUMMARIZATION_MODEL_CONFIG='{"provider": "openai", "model": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo", "base_url": "https://api.together.xyz/v1", "api_key": "your-together-key"}'

# Example 4: Using Groq
# RESEARCH_MODEL_CONFIG='{"provider": "openai", "model": "llama-3.1-70b-versatile", "base_url": "https://api.groq.com/openai/v1", "api_key": "your-groq-key"}'
# SUMMARIZATION_MODEL_CONFIG='{"provider": "openai", "model": "llama-3.1-8b-instant", "base_url": "https://api.groq.com/openai/v1", "api_key": "your-groq-key"}'

# Example 5: Using Azure OpenAI
# RESEARCH_MODEL_CONFIG='{"provider": "openai", "model": "gpt-4o", "base_url": "https://your-resource.openai.azure.com/openai/deployments/gpt-4o", "api_key": "your-azure-key"}'
# COMPRESSION_MODEL_CONFIG='{"provider": "openai", "model": "gpt-4o-mini", "base_url": "https://your-resource.openai.azure.com/openai/deployments/gpt-4o-mini", "api_key": "your-azure-key"}'

# Example 6: Using DeepSeek
# RESEARCH_MODEL_CONFIG='{"provider": "openai", "model": "deepseek-chat", "base_url": "https://api.deepseek.com", "api_key": "your-deepseek-key"}'
# FINAL_REPORT_MODEL_CONFIG='{"provider": "openai", "model": "deepseek-chat", "base_url": "https://api.deepseek.com", "api_key": "your-deepseek-key"}'

# Example 7: Using Anthropic via custom proxy
# RESEARCH_MODEL_CONFIG='{"provider": "openai", "model": "claude-3-5-sonnet-20241022", "base_url": "https://your-proxy.com/anthropic", "api_key": "your-proxy-key"}'

# Example 8: Using Google via custom proxy
# SUMMARIZATION_MODEL_CONFIG='{"provider": "openai", "model": "gemini-1.5-flash", "base_url": "https://your-proxy.com/google", "api_key": "your-proxy-key"}'

# Usage Instructions:
# 1. Choose the examples above that match your use case
# 2. Copy the relevant lines and uncomment them (remove the #)
# 3. Replace the placeholder values (your-xxx-key, your-resource, etc.) with your actual values
# 4. You can mix and match different providers for different models
# 5. If both basic format (RESEARCH_MODEL) and ModelConfig (RESEARCH_MODEL_CONFIG) are provided,
#    the ModelConfig takes precedence

# Additional Configuration
LANGSMITH_API_KEY=
LANGSMITH_PROJECT=
LANGSMITH_TRACING=true

# Should be set to true for a production deployment on Open Agent Platform. Should be set to false otherwise, such as for local development.
GET_API_KEYS_FROM_CONFIG=false